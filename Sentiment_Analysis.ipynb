{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNw8GzwbZWHia6CRZVUcdIW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayhihakim/Data-Science/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqvfSiKsoX68",
        "outputId": "d39747b6-e5cb-4e2f-ecd2-de3db2d8e7cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')  # Download necessary NLTK data\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I love this movie!\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoTY7EchrIvu",
        "outputId": "30e93005-ae40-4375-a4bd-b087c950dbc3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'this', 'movie', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lowercase_tokens = [token.lower() for token in tokens]\n",
        "print(lowercase_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvoX1ywhrRAE",
        "outputId": "3434df72-6c92-485d-9ac2-18cdb59da4e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'love', 'this', 'movie', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(stopwords.words('english'))\n",
        "filtered_tokens = [token for token in lowercase_tokens if token not in stopwords]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ntr9A665rroM",
        "outputId": "ac1d1ea9-913f-4e1b-b22a-f4c4218abaf7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['love', 'movie', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "cleaned_tokens = [re.sub(r'[^\\w\\s]', '', token) for token in filtered_tokens]\n",
        "print(cleaned_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kpSBhtsrvcu",
        "outputId": "0e266786-76c3-4358-a389-33dbbed78dfc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['love', 'movie', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in cleaned_tokens]\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq6BGgz1r48r",
        "outputId": "006db80a-cf49-4474-d496-b6e1d949b839"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['love', 'movi', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "metadata": {
        "id": "1d4QzQpesC1s"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\"I love this movie!\", \"This movie is great.\", \"I don't like this movie.\"]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbDEQL3YsG3O",
        "outputId": "e8b6b3d6-4d2d-4347-882e-3aceabaa6971"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['don' 'great' 'is' 'like' 'love' 'movie' 'this']\n",
            "[[0 0 0 0 1 1 1]\n",
            " [0 1 1 0 0 1 1]\n",
            " [1 0 0 1 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\"I love this movie!\", \"This movie is great.\", \"I don't like this movie.\"]\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g7emdblsLQt",
        "outputId": "d42ec46e-f0e5-4718-ec15-14449fa3bcb7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['don' 'great' 'is' 'like' 'love' 'movie' 'this']\n",
            "[[0.         0.         0.         0.         0.76749457 0.45329466\n",
            "  0.45329466]\n",
            " [0.         0.6088451  0.6088451  0.         0.         0.35959372\n",
            "  0.35959372]\n",
            " [0.6088451  0.         0.         0.6088451  0.         0.35959372\n",
            "  0.35959372]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset from CSV\n",
        "data = pd.read_csv('data.csv')\n",
        "X = data['Sentence']\n",
        "y = data['Sentiment']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature extraction\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_features = vectorizer.fit_transform(X_train)\n",
        "X_test_features = vectorizer.transform(X_test)\n",
        "\n",
        "# Train the SVM classifier\n",
        "clf = svm.SVC()\n",
        "clf.fit(X_train_features, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test_features)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBO7tzjEsQfB",
        "outputId": "dafecc4a-b9fd-458c-cd11-dc615e8714ce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6920444824636441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the dataset from CSV\n",
        "data = pd.read_csv('data.csv')\n",
        "X = data['Sentence']\n",
        "y = data['Sentiment']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature extraction\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_features = vectorizer.fit_transform(X_train)\n",
        "X_test_features = vectorizer.transform(X_test)\n",
        "\n",
        "# Train the SVM classifier\n",
        "clf = svm.SVC()\n",
        "clf.fit(X_train_features, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test_features)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLtMd8NcsgUr",
        "outputId": "8ba3716e-b687-43d5-c7da-21761e8e506d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6920444824636441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "# Split the dataset into features (X) and labels (y)\n",
        "sentences = data['Sentence'].values\n",
        "labels = data['Sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values\n",
        "# Tokenize the sentences\n",
        "tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n"
      ],
      "metadata": {
        "id": "yisibQ52somo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad the sequences\n",
        "padded_sequences = pad_sequences(sequences, padding='post')\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "# Define the neural network architecture\n",
        "vocab_size = len(tokenizer.word_index) + 1 # Added +1 because of reserved 0 index\n",
        "embedding_dim = 100 # You can choose any size for the embedding_dim\n",
        "max_length = len(max(sequences, key=len))\n",
        "model = keras.Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "model.add(LSTM(units=128))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsW5jeGrs-U7",
        "outputId": "2aa063d4-5038-4864-fe54-62eac5a32184"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "147/147 [==============================] - 19s 114ms/step - loss: 0.6291 - accuracy: 0.6833\n",
            "Epoch 2/10\n",
            "147/147 [==============================] - 19s 126ms/step - loss: 0.6272 - accuracy: 0.6833\n",
            "Epoch 3/10\n",
            "147/147 [==============================] - 17s 116ms/step - loss: 0.6264 - accuracy: 0.6833\n",
            "Epoch 4/10\n",
            "147/147 [==============================] - 18s 120ms/step - loss: 0.6254 - accuracy: 0.6833\n",
            "Epoch 5/10\n",
            "147/147 [==============================] - 18s 124ms/step - loss: 0.6254 - accuracy: 0.6833\n",
            "Epoch 6/10\n",
            "147/147 [==============================] - 17s 117ms/step - loss: 0.6253 - accuracy: 0.6833\n",
            "Epoch 7/10\n",
            "147/147 [==============================] - 18s 125ms/step - loss: 0.6253 - accuracy: 0.6833\n",
            "Epoch 8/10\n",
            "147/147 [==============================] - 17s 114ms/step - loss: 0.6250 - accuracy: 0.6833\n",
            "Epoch 9/10\n",
            "147/147 [==============================] - 17s 117ms/step - loss: 0.6259 - accuracy: 0.6833\n",
            "Epoch 10/10\n",
            "147/147 [==============================] - 19s 130ms/step - loss: 0.6254 - accuracy: 0.6833\n",
            "37/37 [==============================] - 2s 28ms/step - loss: 0.6255 - accuracy: 0.6818\n",
            "Loss: 0.625531017780304\n",
            "Accuracy: 0.6817793250083923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "RjB0plpXwqgZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #Load the dataset from CSV\n",
        "data = pd.read_csv('data.csv')\n",
        "# Split the dataset into features (X) and labels (y)\n",
        "X = data['Sentence']\n",
        "y = data['Sentiment']\n",
        "# Convert labels to numerical values\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(y)\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Feature extraction\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_features = vectorizer.fit_transform(X_train)\n",
        "X_test_features = vectorizer.transform(X_test)\n",
        "# Train the model\n",
        "model = SVC()\n",
        "model.fit(X_train_features, y_train)\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_features)"
      ],
      "metadata": {
        "id": "7hEKjzI8xAxh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix)\n",
        "# Hyperparameter tuning using GridSearchCV\n",
        "parameters = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
        "grid_search = GridSearchCV(model, parameters, cv=5)\n",
        "grid_search.fit(X_train_features, y_train)\n",
        "best_model = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu8xurkBxc3P",
        "outputId": "48daa8c3-e2a0-4634-bd08-f74846812a51"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6920444824636441\n",
            "Precision: 0.6579416519780992\n",
            "Recall: 0.6920444824636441\n",
            "F1 Score: 0.6506884603237336\n",
            "Confusion Matrix:\n",
            "[[ 14 115  46]\n",
            " [ 23 567  32]\n",
            " [  1 143 228]]\n",
            "Best Parameters: {'C': 0.1, 'kernel': 'linear'}\n",
            "Best Score: 0.6839242405157508\n"
          ]
        }
      ]
    }
  ]
}